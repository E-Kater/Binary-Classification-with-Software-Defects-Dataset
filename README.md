# Software Defect Prediction - MLOps Project

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Poetry](https://img.shields.io/badge/poetry-managed-cyan.svg)](https://python-poetry.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![MLflow](https://img.shields.io/badge/MLflow-2.4+-orange.svg)](https://mlflow.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)

A comprehensive MLOps project for predicting software defects using machine learning. This project implements a complete ML lifecycle with industry-standard tools and practices.

## ğŸ“‹ Project Overview

This project predicts software defects based on various code metrics using a neural network classifier. It demonstrates a full MLOps pipeline including data processing, model training, experiment tracking, model serving, and CI/CD.

**Business Problem**: Predict whether a software module contains defects based on code metrics (loc, cyclomatic complexity, etc.)

**Dataset**: Software defect prediction dataset from Kaggle (Playground Series S3E23)

## ğŸš€ Features

- **End-to-End MLOps Pipeline**: Data ingestion â†’ Processing â†’ Training â†’ Serving â†’ Monitoring
- **Experiment Tracking**: MLflow for comprehensive experiment management
- **Model Versioning**: DVC for data versioning and reproducibility
- **Automated Workflows**: GitHub Actions CI/CD pipelines
- **Quality Assurance**: Pre-commit hooks, testing, and code formatting
- **Containerization**: Docker support for deployment
- **Model Serving**: Multiple serving options (FastAPI, MLflow, Triton Inference Server)
- **Hyperparameter Tuning**: Optuna integration for optimization

## ğŸ—ï¸ Project Structure

```
software-defect-prediction/
â”œâ”€â”€ .github/workflows/          # GitHub Actions CI/CD pipelines
â”œâ”€â”€ configs/                    # Hydra configuration files
â”‚   â”œâ”€â”€ config.yaml            # Main configuration
â”‚   â”œâ”€â”€ data_config.yaml       # Data processing config
â”‚   â”œâ”€â”€ model_config.yaml      # Model architecture config
â”‚   â””â”€â”€ training_config.yaml   # Training parameters
â”œâ”€â”€ data/                      # Data directory
â”‚   â”œâ”€â”€ raw/                   # Raw data (gitignored)
â”‚   â””â”€â”€ processed/             # Processed data (gitignored)
â”œâ”€â”€ models/                    # Trained models (gitignored)
â”œâ”€â”€ notebooks/                 # Jupyter notebooks for EDA
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â”œâ”€â”€ download_data.py      # Kaggle data download
â”‚   â”œâ”€â”€ data_preprocessing.py # Data processing pipeline
â”‚   â”œâ”€â”€ exploratory_analysis.py # EDA and visualization
â”‚   â””â”€â”€ check_installation.py # Environment verification
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ data/                 # Data handling modules
â”‚   â”‚   â”œâ”€â”€ dataset.py       # PyTorch Dataset
â”‚   â”‚   â””â”€â”€ datamodule.py    # PyTorch Lightning DataModule
â”‚   â”œâ”€â”€ models/              # Model definitions
â”‚   â”‚   â”œâ”€â”€ model.py        # Neural network model
â”‚   â”‚   â””â”€â”€ metric.py       # Custom metrics
â”‚   â”œâ”€â”€ training/           # Training logic
â”‚   â”‚   â””â”€â”€ trainer.py     # Model trainer
â”‚   â”œâ”€â”€ inference/         # Inference utilities
â”‚   â”‚   â””â”€â”€ predictor.py   # Model predictor
â”‚   â”œâ”€â”€ pipelines/         # Main pipelines
â”‚   â”‚   â”œâ”€â”€ train_pipeline.py    # Training pipeline
â”‚   â”‚   â””â”€â”€ inference_pipeline.py # Inference pipeline
â”‚   â””â”€â”€ utils/             # Utility functions
â”‚       â””â”€â”€ logging.py     # Logging configuration
â”œâ”€â”€ tests/                  # Unit tests
â”œâ”€â”€ docker/                # Docker configurations
â”œâ”€â”€ triton/                # Triton Inference Server configs
â”œâ”€â”€ .pre-commit-config.yaml # Pre-commit hooks
â”œâ”€â”€ .dvcignore            # DVC ignore patterns
â”œâ”€â”€ .gitignore           # Git ignore patterns
â”œâ”€â”€ Makefile             # Project commands
â”œâ”€â”€ pyproject.toml       # Poetry dependencies
â”œâ”€â”€ poetry.lock         # Locked dependencies
â””â”€â”€ README.md           # This file
```

## ğŸ› ï¸ Technology Stack

| Category | Tools |
|----------|-------|
| **Language** | Python 3.9+ |
| **ML Framework** | PyTorch, PyTorch Lightning |
| **Configuration** | Hydra, OmegaConf |
| **Experiment Tracking** | MLflow |
| **Data Versioning** | DVC |
| **Dependency Management** | Poetry |
| **Code Quality** | Black, Flake8, isort, mypy |
| **Testing** | pytest, pytest-cov |
| **CI/CD** | GitHub Actions |
| **Containerization** | Docker, Docker Compose |
| **Model Serving** | FastAPI, MLflow, Triton |
| **Hyperparameter Tuning** | Optuna |
| **Monitoring** | Loguru, MLflow Tracking |

## âš™ï¸ Installation

### Prerequisites

- Python 3.9 or higher
- [Poetry](https://python-poetry.org/docs/#installation)
- [Git](https://git-scm.com/)
- [DVC](https://dvc.org/doc/install) (optional, for data versioning)
- [Docker](https://docs.docker.com/get-docker/) (optional, for containerization)

### Quick Start

1. **Clone the repository:**
```bash
git clone https://github.com/yourusername/software-defect-prediction.git
cd software-defect-prediction
```

2. **Install dependencies:**
```bash
make install
```

3. **Set up pre-commit hooks:**
```bash
make setup
```

4. **Activate the virtual environment:**
```bash
poetry shell
```

### Manual Installation

```bash
# Install Poetry if not installed
curl -sSL https://install.python-poetry.org | python3 -

# Clone and setup project
git clone https://github.com/yourusername/software-defect-prediction.git
cd software-defect-prediction

# Install dependencies
poetry install

# Initialize DVC (optional)
dvc init

# Install pre-commit hooks
pre-commit install
```

## ğŸ“Š Dataset

The project uses the Software Defect Prediction dataset from Kaggle Playground Series S3E23.

### Data Features

The dataset contains 21 code metrics:
- `loc`: Lines of code
- `v(g)`: Cyclomatic complexity
- `ev(g)`: Essential complexity
- `iv(g)`: Design complexity
- `n`: Halstead program length
- `v`: Halstead volume
- `l`: Halstead program level
- `d`: Halstead difficulty
- `i`: Halstead intelligence
- `e`: Halstead effort
- `b`: Halstead bugs
- `t`: Halstead time estimator
- `lOCode`: Lines of code
- `lOComment`: Lines of comments
- `lOBlank`: Blank lines
- `locCodeAndComment`: Code and comment lines
- `uniq_Op`: Unique operators
- `uniq_Opnd`: Unique operands
- `total_Op`: Total operators
- `total_Opnd`: Total operands
- `branchCount`: Number of branches

**Target variable**: `defects` (TRUE/FALSE)

### Downloading Data

```bash
# Option 1: Using Kaggle API (recommended)
make download

# Option 2: Manual download
# 1. Download from Kaggle: https://www.kaggle.com/competitions/playground-series-s3e23
# 2. Place the CSV file in data/raw/defects.csv
```

## ğŸš€ Usage

### Complete Pipeline

Run the entire MLOps pipeline:

```bash
make full-pipeline
```

This will:
1. Download data from Kaggle
2. Preprocess and split the data
3. Train the model with experiment tracking
4. Evaluate on test set
5. Save the best model

### Individual Steps

#### 1. Data Processing
```bash
make preprocess
```

#### 2. Exploratory Data Analysis
```bash
make explore
```

#### 3. Train Model
```bash
make train
# or with custom parameters
python src/pipelines/train_pipeline.py model.learning_rate=0.0005 model.hidden_sizes="[128,64]"
```

#### 4. Run Inference
```bash
make predict
```

#### 5. Launch MLflow UI
```bash
make mlflow
# Open http://localhost:5000 in your browser
```

### Model Training Options

#### Basic Training
```bash
python src/pipelines/train_pipeline.py
```

#### Training with Class Weights (for imbalanced data)
```bash
python src/pipelines/train_pipeline.py data.use_class_weights=true
```

#### Hyperparameter Tuning
```bash
python scripts/hyperparameter_tuning.py
```

#### Training Improved Model
```bash
python src/pipelines/train_improved.py
```

## ğŸ”§ Configuration

The project uses Hydra for configuration management. Key configuration files:

- **`configs/config.yaml`**: Main configuration
- **`configs/data_config.yaml`**: Data processing settings
- **`configs/model_config.yaml`**: Model architecture
- **`configs/training_config.yaml`**: Training parameters

### Example Configuration Override

```bash
# Train with custom parameters
python src/pipelines/train_pipeline.py \
    model.learning_rate=0.0005 \
    model.hidden_sizes="[128,64,32]" \
    model.dropout_rate=0.4 \
    training.max_epochs=100
```

## ğŸ“ˆ Model Architecture

The project implements a neural network classifier:

```
Input (21 features)
    â†“
Linear(21 â†’ 128)
    â†“
ReLU Activation
    â†“
Dropout(0.3)
    â†“
Linear(128 â†’ 64)
    â†“
ReLU Activation
    â†“
Dropout(0.3)
    â†“
Linear(64 â†’ 32)
    â†“
ReLU Activation
    â†“
Dropout(0.3)
    â†“
Linear(32 â†’ 2)
    â†“
Output (defect/no defect)
```

### Key Features

- **Class Weighting**: Handles imbalanced datasets
- **Early Stopping**: Prevents overfitting
- **Learning Rate Scheduling**: Adaptive learning rate
- **Gradient Clipping**: Improves training stability
- **Comprehensive Metrics**: Accuracy, Precision, Recall, F1, ROC-AUC

## ğŸ§ª Testing

Run tests to ensure code quality:

```bash
# Run all tests
make test

# Run specific test file
pytest tests/test_model.py -v

# Run with coverage report
pytest --cov=src tests/ --cov-report=html
```

## ğŸ¯ Model Serving

### Option 1: FastAPI REST API
```bash
make api
# API available at http://localhost:8000
# Swagger docs at http://localhost:8000/docs
```

### Option 2: MLflow Serving
```bash
# Serve a specific model version
make mlflow-serve MODEL_URI=runs:/<run_id>/model
# Available at http://localhost:8001
```

### Option 3: Triton Inference Server (Production)
```bash
# Convert model to Triton format
make triton-convert

# Start Triton server
make triton-start

# Test Triton client
make triton-test
```

### Inference Examples

```python
# Python client example
from src.inference.predictor import DefectPredictor

predictor = DefectPredictor("models/best_model.ckpt")
result = predictor.predict_single({
    "loc": 22.0,
    "v(g)": 3.0,
    # ... other features
})
```

```bash
# REST API call
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{
       "loc": 22.0,
       "v(g)": 3.0,
       "ev(g)": 1.0,
       "iv(g)": 2.0,
       "n": 60.0,
       "v": 278.63,
       "l": 0.06,
       "d": 19.56,
       "i": 14.25,
       "e": 5448.79,
       "b": 0.09,
       "t": 302.71,
       "lOCode": 17,
       "lOComment": 1,
       "lOBlank": 1,
       "locCodeAndComment": 0,
       "uniq_Op": 16.0,
       "uniq_Opnd": 9.0,
       "total_Op": 38.0,
       "total_Opnd": 22.0,
       "branchCount": 5.0
     }'
```

## ğŸ“Š Experiment Tracking with MLflow

MLflow is integrated for comprehensive experiment tracking:

```bash
# Start MLflow UI
make mlflow

# View experiments at http://localhost:5000
```

### Tracked Information

- **Parameters**: All configuration parameters
- **Metrics**: Training and validation metrics per epoch
- **Artifacts**: Models, logs, configuration files
- **Tags**: Experiment metadata
- **Plots**: Training curves, confusion matrices

## ğŸ”„ CI/CD Pipeline

GitHub Actions workflows automate:

1. **Code Quality Checks**: Linting, formatting, type checking
2. **Unit Tests**: Automated testing with coverage
3. **Integration Tests**: End-to-end pipeline testing
4. **Model Training**: Scheduled retraining
5. **Deployment**: Docker image building and pushing

## ğŸ³ Docker Support

### Build and Run

```bash
# Build Docker image
docker build -t software-defect-prediction .

# Run container
docker run -p 8000:8000 software-defect-prediction

# Docker Compose (with MLflow)
docker-compose up
```

### Development with Docker

```bash
# Development environment
docker-compose -f docker/development.yml up

# Production deployment
docker-compose -f docker/production.yml up
```

## ğŸ“ Data Versioning with DVC

Track datasets and models with DVC:

```bash
# Track data files
dvc add data/raw/defects.csv

# Track processed data
dvc add data/processed/train.csv data/processed/test.csv

# Push to remote storage
dvc push

# Pull data
dvc pull
```

## ğŸ§¹ Code Quality

### Pre-commit Hooks

Automated checks run before each commit:

```bash
# Install hooks
make setup

# Run manually
make lint

# Auto-fix issues
black src/ tests/ scripts/
isort src/ tests/ scripts/
```

### Code Formatting Standards

- **Black**: Code formatting (88 char line length)
- **isort**: Import sorting
- **Flake8**: Code style checking
- **mypy**: Type checking
- **pre-commit**: Automated git hooks

## ğŸ“ˆ Performance Monitoring

### Training Monitoring

```bash
# Monitor training logs
make monitor

# View MLflow metrics in real-time
make mlflow
```

### Model Performance

Key metrics tracked:
- **Accuracy**: Overall prediction correctness
- **Precision**: Quality of positive predictions
- **Recall**: Coverage of actual positives
- **F1-Score**: Balance of precision and recall
- **ROC-AUC**: Model discrimination ability
- **Inference Latency**: Prediction speed

## ğŸ” Debugging

### Common Issues

1. **Missing Data**: Ensure data is in `data/raw/defects.csv`
2. **CUDA Errors**: Set `accelerator: cpu` in config for CPU-only machines
3. **Import Errors**: Run `poetry install` to ensure all dependencies
4. **Memory Issues**: Reduce `batch_size` in config

### Debug Commands

```bash
# Check installation
make check

# Test individual components
python scripts/check_model.py
python scripts/check_data.py

# Verbose logging
python src/pipelines/train_pipeline.py --verbose
```

## ğŸ“š Documentation

### Generated Documentation

```bash
# Generate API documentation
pdoc --html src --output-dir docs/

# Generate project documentation
mkdocs build
```

### Code Documentation

- Docstrings follow Google style
- Type hints throughout codebase
- README files in each module

## ğŸ¤ Contributing

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/amazing-feature`
3. **Commit changes**: `git commit -m 'Add amazing feature'`
4. **Push to branch**: `git push origin feature/amazing-feature`
5. **Open a Pull Request**

### Development Guidelines

- Write tests for new features
- Update documentation
- Follow code style guidelines
- Add type hints
- Update dependencies in `pyproject.toml`

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Kaggle for the dataset
- PyTorch and PyTorch Lightning teams
- MLflow for experiment tracking
- Hydra for configuration management
- All open-source contributors

## ğŸ“ Support

For questions and support:

1. **Issues**: [GitHub Issues](https://github.com/yourusername/software-defect-prediction/issues)
2. **Discussions**: [GitHub Discussions](https://github.com/yourusername/software-defect-prediction/discussions)
3. **Email**: your.email@example.com

## ğŸ“Š Project Status

| Component | Status | Notes |
|-----------|--------|-------|
| Data Pipeline | âœ… Complete | Kaggle integration working |
| Model Training | âœ… Complete | PyTorch Lightning implemented |
| Experiment Tracking | âœ… Complete | MLflow fully integrated |
| Model Serving | âœ… Complete | Multiple serving options |
| CI/CD | âœ… Complete | GitHub Actions workflows |
| Documentation | âœ… Complete | Comprehensive README |
| Testing | âœ… Complete | 90%+ coverage |
| Deployment | ğŸŸ¡ In Progress | Docker images ready |
| Monitoring | ğŸŸ¡ In Progress | Basic monitoring implemented |

## ğŸ”® Future Enhancements

- [ ] Real-time prediction API
- [ ] Model monitoring dashboard
- [ ] A/B testing framework
- [ ] Automated retraining pipeline
- [ ] Feature store integration
- [ ] Multi-model ensemble
- [ ] Explainable AI (SHAP/LIME)
- [ ] Automated data drift detection

---

**â­ If you find this project useful, please give it a star! â­**
